---
permalink: /
title: "Overview " # ([Curriculum Vitae](https://lijian.ac.cn/files/cv/UCAS_PhD_lijian.pdf))
excerpt: "Overview"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am an Associate Senior Researcher (Tenure-track Senior Researcher) and Master's supervisor at the Institute of Information Engineering, Chinese Academy of Sciences (CAS), working on large-scale statistical learning theory and large language models (LLMs).
In July 2020, I obtained my Ph.D. degree from Institute of Information Engineering, CAS, advised by Associate Prof. [Yong Liu](https://liuyonggsai.github.io/) and Prof. Weiping Wang. 

With the goal of bridging the gap between theories and algorithms, I am working to develop **theoretical guarantees** and **efficient algorithms** for LLMs and other machine learning methods.
My research interests include, but are not limited to:
* **LLMs and Deep Learning Theory**: theoretical studies on the unique capabilities of large language models (such as emergent abilities and grokking), as well as benign overfitting or the double descent phenomenon in deep learning.
* **Efficient LLMs**: efficient Transformers, compressed LLMs, and parameter-efficient fine-tuning (PEFT).
* **Large-scale Machine Learning**: statistical guarantees and improved algorithms for large-scale machine learning methods, including federated learning, distributed learning, random features, Nystr√∂m methods, sketching, etc.
  

# Selected Papers [[Full List](https://lijian.ac.cn/publications/)] [[Google Scholar](https://scholar.google.com/citations?hl=zh-CN&user=IAJpTqYAAAAJ&view_op=list_works&sortby=pubdate)] 
<i>* corresponding author</i>
## Preprint

* A Survey on Model Compression for Large Language Models.
[[pdf]](https://arxiv.org/abs/2308.07633) <br>
Xunyu Zhu, <u><b>Jian Li*</b></u>, Yong Liu, Can Ma, Weiping Wang.  <br>
<i> arXiv:2308.07633</i>. 

## Journal Papers

* Optimal Convergence Rates for Distributed Nystr√∂m Approximation. 
[[pdf]](https://jmlr.org/papers/volume24/21-1049/21-1049.pdf)
[[code]](https://github.com/superlj666/DNystroem) <br>
<u><b>Jian Li</b></u>, Yong Liu, Weiping Wang. <br>
<i>Journal of Machine Learning Research</i> (**JMLR**), 2023. <b>CCF-A</b>.

* Optimal Convergence for Agnostic Kernel Learning With Random Features.
[[pdf]](https://ieeexplore.ieee.org/abstract/document/10304308)
[[code]](https://github.com/superlj666/Agnostic-RF) <br>
<u><b>Jian Li</b></u>, Yong Liu, Weiping Wang.  <br>
<i>IEEE Transactions on Neural Networks and Learning Systems</i> (**TNNLS**), 2023. <b>CCF-B</b>.

* Semi-supervised vector-valued learning: Improved bounds and algorithms. 
[[pdf]](https://www.sciencedirect.com/science/article/pii/S0031320323000572) <br>
<u><b>Jian Li</b></u>, Yong Liu, Weiping Wang.  <br>
<i>Pattern Recognition</i> (**PR**), 2023. <b>CCF-B</b>.

* Improving Differentiable Architecture Search via Self-distillation.
[[pdf]](https://doi.org/10.1016/j.neunet.2023.08.062) <br>
Xunyu Zhu, <u><b>Jian Li*</b></u>, Yong Liu, Weiping Wang.  <br>
<i>Neural Networks</i>, 2023. <b>CCF-B</b>.

* Convolutional Spectral Kernel Learning with Generalization Guarantees.
[[Paper]](https://doi.org/10.1016/j.artint.2022.103803)
[[Code]](https://github.com/superlj666/CSKN/) <br>
<u><b>Jian Li</b></u>, Yong Liu, Weiping Wang. <br>
<i>Artificial Intelligence</i> (**AIJ**), 2022. <b>CCF-A</b>.

* Non-IID Federated Learning with Sharper Risk Bound.
[[pdf]](https://doi.org/10.1109/TNNLS.2022.3213187) <br>
Bojian Wei, <u><b>Jian Li*</b></u>, Yong Liu, Weiping Wang.  <br>
<i>IEEE Transactions on Neural Networks and Learning Systems</i> (**TNNLS**), 2022. <b>CCF-B</b>.

## Conference Papers

* Optimal Convergence Rates for Agnostic Nystr√∂m Kernel Learning.
[[pdf]](https://openreview.net/pdf?id=S3d9SwhRKh) <br>
<u><b>Jian Li</b></u>, Yong Liu, Weiping Wang. <br>
<i>International Conference on Machine Learning </i> (**ICML**), 2023. <b>CCF-A</b>.

* Towards Sharp Analysis for Distributed Learning with Random Features. [[pdf]](https://www.ijcai.org/proceedings/2023/0436.pdf) <br>
<u><b>Jian Li</b></u>, Yong Liu. <br>
<i>International Joint Conference on Artificial Intelligence</i> (**IJCAI**), 2023. <b>CCF-A</b>.

* Ridgeless Regression with Random Features.
[[pdf]](https://arxiv.org/pdf/2205.00477.pdf)
[[code]](https://github.com/superlj666/Ridgeless-Regression-with-Random-Features) <br>
<u><b>Jian Li</b></u>, Yong Liu, Yingying Zhang. <br>
<i>International Joint Conference on Artificial Intelligence</i> (**IJCAI**), 2022. <b>CCF-A</b>.

* Federated learning for non-iid data: From theory to algorithm. 
[[pdf]](https://lijian.ac.cn/files/2021/FL_for_noniid_data.pdf)
[[presentation]](https://lijian.ac.cn/files/2021/FL_for_noniid_data_presentation.pdf)
üèÜ<u><b>[[Best student paper award]](https://lijian.ac.cn/files/2021/PRICAI-2021-best-student-paper.png)</b></u><br>
Bojian Wei, <u><b>Jian Li*</b></u>, Yong Liu, Weiping Wang. <br>
<i>Pacific Rim International Conference on Artificial Intelligence</i> (**PRICAI**), 2021. CCF-C.

* Automated Spectral Kernel Learning. 
[[pdf]](https://ojs.aaai.org/index.php/AAAI/article/view/5892/5748)
[[poster]](https://lijian.ac.cn/files/2020_AAAI_ASKL/2020_AAAI_AKSL_poster.pdf)
[[code]](https://github.com/superlj666/Automated-Spectral-Kernel-Learning) <br>
<u><b>Jian Li</b></u>, Yong Liu, Weiping Wang. <br>
<i>AAAI Conference on Artificial Intelligence</i> (**AAAI**), 2020. <b>CCF-A</b>.

* Multi-Class Learning: From Theory to Algorithm. 
[[pdf]](https://proceedings.neurips.cc/paper/2018/file/1141938ba2c2b13f5505d7c424ebae5f-Paper.pdf)
[[poster]](https://lijian.ac.cn/files/2018_NeurIPS_MC/mc-lrc-nips-poster.pdf)
[[sildes]](https://lijian.ac.cn/files/2018_NeurIPS_MC/mc-lrc-nips-slides.pdf)
[[3-minute video]](https://youtu.be/mE_RpgWuKK8)
[[code]](https://github.com/superlj666/Multi-Class-Learning-From-Theory-to-Algorithm) <br>
<u><b>Jian Li</b></u>, Yong Liu, Rong Yin, Hua Zhang, Lizhong Ding, Weiping Wang. <br>
<i>Advances in Neural Information Processing Systems 31</i> (**NeurIPS**), 2018. <b>CCF-A</b>.

#  Projects
* China Postdoctoral Science Foundation (No. 2023T160680), 2023.07 - 2024.03, &yen;180,000. <br>
<i>Research on Deep Differentiable Gaussian Processes for Structured Prediction</i>.

* National Key R&D Program of China (2022YFB3105302.2), 2022.12 - 2025.11, &yen;1,200,000. <br>
<i> Aggregation and Collaborative Techniques for Cross-platforms Heterogenous Data</i>.

* National Natural Science Foundation of China (No. 62106257), 202201 - 2024.12, &yen;300,000. <br>
<i> Large Scale Structured Prediction with Automated Spectral Kernel Learning</i>.

* Special Research Assistant Project of CAS, 2020.09 - 2022.09, &yen;800,000. <br>
<i> Large-scale Few-shot Automated Machine Learning</i>.

* Excellent Talents Program of Institute of Information Engineering, CAS, 2020.09 - 2026.09. <br>
<i> Tenure-track Senior Researcher (Professor)</i>.

# Patents

## Application

* Jian Li, Yong Liu, Liubin Wang, Yiguo Yang, Juhong Wang.Neural Network Architecture Search Method, Device, Computer Equipment, and Storage Medium: CN. Application numberÔºö202011567991.3. December 25, 2020.
* Jian Li, Jiaoyang Li, Bojian Wei, Yong Liu, Weiping Wang. A Federated Learning Method and System Based on Attention Mechanism: CN. Application number: 202311073645.3. August 24, 2023
* Jian Li, Jiaoyang Li, Zheng Lin, Yong Liu, Weiping Wang. A Vertical Domain Large Model Method and System Based on Knowledge Distillation and Prompt Engineering: CN. Application number: 202311073641.5. August 24, 2023.

## Granted

* Hailun Lin, Yong Liu, Jian Li, Weiping Wang. A Large-Scale Ontology Merging Method that Integrates Representation Learning and Divide-and-Conquer Strategy: China. Patent number: CN110059194A. April 8, 2022.

# Curriculum Vitae

| Time               | Title                                                       | Institution                               | Research Direction                                    |
|:-------------------| :---------------------------------------------------------- | :---------------------------------------- | :---------------------------------------------------- |
| 2023.11 - present  | Associate Senior Researcher, Tenure-track Senior Researcher | Institute of Information Engineering, CAS | LLMs, Large-scale Statistical Machine Learning  |
| 2020.09 - 2023.11  | Postdoctoral Researcher, Tenure-track Senior Researcher     | Institute of Information Engineering, CAS | Large-scale Statistical Machine Learning              |
| 2015.09 - 2020.07  | Ph.D. Candidate                                             | Institute of Information Engineering, CAS | Large-scale Model Selection, Semi-supervised Learning |
| 2011.09 - 2020.07  | Bachelor Candidate                                          | Northeastern University                   | Software Engineering (International class)            |

# Students
- Ph.D. students
  - üéìYilin Kang (2018-2023), Differential Privacy. Papers: Computers & Security, CIKM 2022, ICCS 2023. Post-graduation: Researcher in Purple Mountain Laboratories.
  - Xunyu Zhu (2020-present), Neural Architecture Search. Papers: ICDM 2021.
  - Boxuan Che (2020-present), Efficient Graph Neural Networks.
- Master's students
  - üéìBojian Wei (2019-2022), Federated Learning on Heterogenous Data. Papers: PRICAI 2021 (**best student paper award**), ECML-PKDD 2022, TNNLS, IJCNN 2023. Post-graduation: Management Trainee in Bank of China Head Office.
  - Xuning Zhang (2023-present), Federated Learning. **Excellent Bachelor's Thesis in Wuhan University**.

# Honors and Awards
* PRICAI 2021 best student paper award.
* Outstanding Graduates of Beijing, 2020.
* Outstanding Graduates of University of Chinese Academy of Sciences (UCAS), 2020.
* Outstanding Graduates of Institute of Information Engineering, CAS, 2020.
* National Scholarship for Doctoral students, 2019.
* ZhuLiYueHua Scholarship for Excellent Doctoral Student, 2019.
* CAS Presidential Scholarship, 2019.
* National Scholarship for Doctoral students, 2018.
* IIE Presidential Scholarship, 2018.

# Academic Service
- Program committee/reviewer:
  - Conferences: ICML, NeurIPS, ICLR, AAAI, IJCAI, ECAI
  - Journals: IEEE TPAMI, JMLR, Pattern Recognition