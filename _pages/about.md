---
permalink: /
# title: "Overview " # ([Curriculum Vitae](https://lijian.ac.cn/files/cv/UCAS_PhD_lijian.pdf))
# excerpt: "Overview"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am an Associate Professor and Master's supervisor at the Institute of Information Engineering, Chinese Academy of Sciences (CAS), working on large-scale statistical learning theory and large language models (LLMs).
In July 2020, I obtained my Ph.D. degree from Institute of Information Engineering, CAS, advised by Associate Prof. [Yong Liu](https://liuyonggsai.github.io/) and Prof. Weiping Wang. 


My research focuses on foundational machine learning theory, particularly the generalization theory of large-scale methods. Addressing the lag in foundational theory compared to empirical algorithms in large-scale machine learning, I aim to uncover the underlying principles and narrow the gap between theory and practical algorithms. Ultimately, I strive to guide large-scale algorithm design for a balance between computational efficiency and generalization performance. Specific interests include:

- **Optimal Generalization Guarantees for Large-Scale ML**: Investigating optimal generalization guarantees, relaxing assumptions, and enhancing large-scale algorithms, including federated learning, distributed learning, and random features.

- **Generalization Theory of Deep Neural Networks**: Exploring connections between neural networks and kernel methods, studying generalization in non-stationary spectral kernel networks, refining current neural network models, and using random matrix theory to understand phenomena in deep networks.

- (Future Direction) **Fundamental Research on Large Language Models**: Delving into the foundational theory of large language models, explaining unique capabilities like scaling laws, context learning, and complex reasoning. Improving model architecture for computational efficiency and performance and researching the next generation of efficient language models with reduced parameters.

## Curriculum Vitae ([CV](https://lijian.ac.cn/files/cv/JianLi_CV.pdf))

### Career

|  Institution  |  Title    |  Time |
|:------------------- | :----------------------------- |:---------------------- 
| Institute of Information Engineering, CAS  | Associate Professor          | 2023.10 - present     
| Microsoft Research Asia (NLC Group) | Visiting Scholar | 2024.04 - 2024.06  
| Institute of Information Engineering, CAS | Tenure-track Assistant Professor | 2020.09 - 2023.10  

### Education

|   Institution  |  Major  |  Degree  |   Time   |
|:------------------- | :----------------------------- |:---------------------- |:---------------------- |
| University of Chinese Academy of Sciences (UCAS)  |  Cyber Security             |  Ph.D.                 | 2015.09 - 2020.06 |
| Northeastern University        | Software Engineering (International class)      |  Bachelor                        | 2011.09 - 2015.06 |

## Selected Papers [[Full List](https://lijian.ac.cn/publications/)] [[Google Scholar](https://scholar.google.com/citations?hl=en-us&user=IAJpTqYAAAAJ&view_op=list_works&sortby=pubdate)] 

* A Survey on Model Compression for Large Language Models. 
[[pdf]](https://arxiv.org/pdf/2308.07633)
<br>
<u><b>Jian Li</b></u>, Yong Liu, Weiping Wang. <br>
<i>Transactions of the Association for Computational Linguistics</i> (**TACL**), 2024, Â∑≤ÂΩïÁî®. <b>CCF-B ÊúüÂàä</b>. <br>

* Optimal Rates for Agnostic Distributed Learning. 
[[pdf]](https://ieeexplore.ieee.org/document/10365227)
[[code]](https://github.com/superlj666/Agnostic-DKRR)
<br>
<u><b>Jian Li</b></u>, Yong Liu, Weiping Wang. <br>
<i>IEEE Transactions On Information Theory</i> (**TIT**), 2023. <b>CCF-A Journal</b>. <br>

* Optimal Convergence Rates for Distributed Nystr√∂m Approximation. 
[[pdf]](https://jmlr.org/papers/volume24/21-1049/21-1049.pdf)
[[code]](https://github.com/superlj666/DNystroem) <br>
<u><b>Jian Li</b></u>, Yong Liu, Weiping Wang. <br>
<i>Journal of Machine Learning Research</i> (**JMLR**), 2023. <b>CCF-A Journal</b>.

* Convolutional Spectral Kernel Learning with Generalization Guarantees.
[[pdf]](https://doi.org/10.1016/j.artint.2022.103803)
[[code]](https://github.com/superlj666/CSKN/) <br>
<u><b>Jian Li</b></u>, Yong Liu, Weiping Wang. <br>
<i>Artificial Intelligence</i> (**AI**), 2022. <b>CCF-A Journal</b>.

* Optimal Convergence Rates for Agnostic Nystr√∂m Kernel Learning.
[[pdf]](https://openreview.net/forum?id=S3d9SwhRKh)
<br>
<u><b>Jian Li</b></u>, Yong Liu, Weiping Wang. <br>
<i>International Conference on Machine Learning </i> (**ICML**), 2023. <b>CCF-A Conference</b>. 


* Multi-Class Learning: From Theory to Algorithm. 
[[pdf]](https://proceedings.neurips.cc/paper/2018/file/1141938ba2c2b13f5505d7c424ebae5f-Paper.pdf)
[[poster]](https://lijian.ac.cn/files/2018_NeurIPS_MC/mc-lrc-nips-poster.pdf)
[[sildes]](https://lijian.ac.cn/files/2018_NeurIPS_MC/mc-lrc-nips-slides.pdf)
[[3-minute video]](https://youtu.be/mE_RpgWuKK8)
[[code]](https://github.com/superlj666/Multi-Class-Learning-From-Theory-to-Algorithm) <br>
<u><b>Jian Li</b></u>, Yong Liu, Rong Yin, Hua Zhang, Lizhong Ding, Weiping Wang. <br>
<i>Advances in Neural Information Processing Systems 31</i> (**NeurIPS**), 2018. <b>CCF-A Conference</b>.

* Federated learning for non-iid data: From theory to algorithm. 
[[pdf]](https://link.springer.com/chapter/10.1007/978-3-030-89188-6_3)
[[presentation]](https://lijian.ac.cn/files/2021/FL_for_noniid_data_presentation.pdf)
[[üèÜ<b>ÊúÄ‰Ω≥Â≠¶ÁîüËÆ∫ÊñáÂ•ñ</b>]](https://lijian.ac.cn/files/2021/PRICAI-2021-best-student-paper.png) (1/92)<br>
Bojian Wei, <u><b>Jian Li*</b></u>, Yong Liu, Weiping Wang. <br>
<i>Pacific Rim International Conference on Artificial Intelligence</i> (**PRICAI**), 2021. CCF-C Conference.

##  Projects
* National Key R&D Program of China (2022YFB3105302.2), 2022.12 - 2025.11, &yen;1,200,000. <br>
<i> Aggregation and Collaborative Techniques for Cross-platforms Heterogenous Data</i>.

* National Natural Science Foundation of China (No. 62106257), 2022.01 - 2024.12, &yen;300,000. <br>
<i> Large Scale Structured Prediction with Automated Spectral Kernel Learning</i>.

* China Postdoctoral Science Foundation (**Special Support**, No. 2023T160680), 2023.07 - 2024.03, &yen;180,000. <br>
<i>Research on Deep Differentiable Gaussian Processes for Structured Prediction</i>.

* Special Research Assistant Project of CAS, 2020.09 - 2022.09, &yen;800,000. <br>
<i> Large-scale Few-shot Automated Machine Learning</i>.

* Talent Program Class A of Institute of Information Engineering, CAS, Tenure-track Professor, 2023.10 - 2026.09.

* Talent Program Class B of Institute of Information Engineering, CAS, Tenure-track Young Professor, 2020.09 - 2023.10.


## Patents


### Granted

* Hailun Lin, Yong Liu, <u><b>Jian Li</b></u>, Weiping Wang. A Large-Scale Ontology Merging Method that Integrates Representation Learning and Divide-and-Conquer Strategy: China. Granted No.CN110059194A. Granted Date: April 8, 2022.
  
### Pending

* <u><b>Jian Li</b></u>, Yong Liu, Liubin Wang, Yiguo Yang, Juhong Wang.Neural Network Architecture Search Method, Device, Computer Equipment, and Storage Medium. CNÔºö202011567991.3. App. Date: December 25, 2020.
* <u><b>Jian Li</b></u>, Jiaoyang Li, Bojian Wei, Yong Liu, Weiping Wang. A Federated Learning Method and System Based on Attention Mechanism. CN: 202311073645.3. App. Date: August 24, 2023
* <u><b>Jian Li</b></u>, Jiaoyang Li, Zheng Lin, Yong Liu, Weiping Wang. A Vertical Domain Large Model Method and System Based on Knowledge Distillation and Prompt Engineering. CN: 202311073641.5. App. Date: August 24, 2023.


## Students
- Ph.D. students
  - üéìYilin Kang (2020.09 - 2023.06 ), Differential Privacy. </br>Publications: Computers & Security, CIKM 2022, ICCS 2023. </br> Post-graduation: Researcher in Purple Mountain Laboratories.
  - Xunyu Zhu (2020.09 - present), Efficient LLM Inference. </br>Publications: ICDM 2021, Neural Networks, TACL. </br>In submissions: TACL, Neural Networks, TNNLS.
  - Boxuan Che (2022.09 - present), Efficient Graph Neural Networks.
- Master students
  - üéìBojian Wei (2020.09 - 2022.06), Federated Learning on Heterogenous Data. </br>Publications: PRICAI 2021 (**best student paper award**), ECML-PKDD 2022, TNNLS, IJCNN 2023. </br>Post-graduation: Management Trainee in Bank of China Head Office.
  - Xuning Zhang (2022.09 - present), Federated Learning. </br>**Excellent Bachelor's Thesis in Wuhan University in 2023**.

## Honors and Awards
* Microsoft Research Asia StarTrack Scholars Program, 2024
* Talent Plan Class A of IIE, CAS, 2023.
* PRICAI 2021 best student paper award, 2021.
* Special Research Assistant of Chinese Academy of Sciences, 2020.
* Talent Plan Class B of IIE, CAS, 2020.
* ~~AIDU Talents of Baidu Research (Decline)~~, 2020
* ~~Joint Ph.D. Program with Stanford University (Discontinued due to COVID-19)~~, 2020.02 - 2021.02.
* Outstanding Graduates of Beijing, 2020.
* Outstanding Graduates of University of Chinese Academy of Sciences (UCAS), 2020.
* Outstanding Graduates of Institute of Information Engineering, CAS, 2020.
* National Scholarship for Doctoral students, 2019.
* ZhuLiYueHua Scholarship for Excellent Doctoral Student, 2019.
* CAS Presidential Scholarship, 2019.
* National Scholarship for Doctoral students, 2018.

## Academic Service
* Mathematics Guest Editor
* Program committee of Conference: ICML, NeurIPS, ICLR, AAAI, IJCAI, ECAI, etc.
* Reviewers of Journals: TPAMI, JMLR, Pattern Recognition, etc.